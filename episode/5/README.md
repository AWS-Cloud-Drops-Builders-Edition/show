# Epis√≥dio 6 - Ajustando LLMs com Parameter efficient fine-tunning (PEFT)

[![YouTube video thumbnail](./thumb.jpeg)](https://www.youtube.com/watch?v=18eWp6ceP4k)
**[&#x25b6; Assista agora no Youtube!](https://www.youtube.com/watch?v=18eWp6ceP4k)**

Como vimos no √∫ltimo epis√≥dio, o ajuste de LLMs √© computacionalmente intensivo. O ajuste fino completo requer mem√≥ria n√£o apenas para armazenar o modelo, mas v√°rios outros par√¢metros que s√£o necess√°rios durante o processo de treinamento. Mesmo que o seu computador possa suportar o tamanho do modelo, que agora √© da ordem de centenas de gigabytes para os modelos maiores, voc√™ tamb√©m deve ser capaz de alocar mem√≥ria para v√°rios componentes adjacentes e mem√≥ria tempor√°ria durante todo o processo de treinamento. Esses componentes adicionais podem ser muitas vezes maiores que o modelo e podem rapidamente se tornar grandes demais para serem manuseados em hardware de uso geral. 

Neste epis√≥dio, veremos alternativas ao ajuste fino completo, utilizando o Parameter efficient fine-tuning (PEFT), para utilizar t√©cnicas que atualizam apenas um pequeno subconjunto de par√¢metros. 

## O que mencionamos durante o epis√≥dio

## üêõ Bugs em produ√ß√£o

## ü´∞ D√≠vida t√©cnica

## Onde aprender mais

* [C√≥digo utilizado neste epis√≥dio (guithub)](https://github.com/AWS-Cloud-Drops-Builders-Edition/s01e06)
* [PartyRocket - build web apps with gen AI](https://partyrock.aws/u/mza/pDK3iF1kb/PartyRocket-build-web-apps-with-gen-AI/snapshot/xKLiQfwHd)
* [AWS re:Invent 2023 - Keynote with Dr. Werner Vogels](https://www.youtube.com/watch?v=UTRBVPvzt9w)
* [HELM is a living benchmark to evaluate Language Models more transparently](https://crfm.stanford.edu/helm/latest/)
* [Este artigo apresenta o GLUE, uma refer√™ncia para avaliar modelos em diversas tarefas de compreens√£o de linguagem natural (NLU) e enfatiza a import√¢ncia de sistemas NLU gerais aprimorados](https://openreview.net/pdf?id=rJ4km2R5t7)
* [Este artigo apresenta o SuperGLUE, um benchmark projetado para avaliar o desempenho de v√°rios modelos de PNL em uma s√©rie de tarefas desafiadoras de compreens√£o de linguagem](https://super.gluebenchmark.com/)
* [Este artigo apresenta e avalia quatro medidas diferentes (ROUGE-N, ROUGE-L, ROUGE-W e ROUGE-S) no pacote de avalia√ß√£o de resumo ROUGE, que avalia a qualidade dos resumos comparando-os com resumos ideais gerados por humanos](https://aclanthology.org/W04-1013.pdf)
* [Este artigo apresenta um novo teste para medir a precis√£o multitarefa em modelos de texto, destacando a necessidade de melhorias substanciais para alcan√ßar a precis√£o de n√≠vel especializado e abordar o desempenho desequilibrado e a baixa precis√£o em assuntos socialmente importantes](https://arxiv.org/pdf/2009.03300.pdf)
* [O artigo apresenta o BIG-bench, um benchmark para avaliar modelos de linguagem em tarefas desafiadoras, fornecendo insights sobre escala, calibra√ß√£o e preconceito social](https://arxiv.org/pdf/2206.04615.pdf)


[![AWS PartyRock](../../images/partyrock.jpeg)](https://partyrock.aws/)
**[B√≥ra construir uma aplica√ß√£o baseada em IA](https://partyrock.aws/)**

[![Generative AI with LLMs](../../images/Generative%20AI%20with%20LLMs.jpeg)](https://www.deeplearning.ai/courses/generative-ai-with-llms/)
**[DeepLearning.AI](https://www.deeplearning.ai/courses/generative-ai-with-llms/)**
